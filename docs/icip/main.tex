% Template for ICIP-2014 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{SVETLANA: a Supervised Segmentation Classifier for Napari}
%
% - STALIN
% - SALSIFI
% - SALSA
% - SCAN
% 
% ---------------
\name{Clément Cazorla, Pierre Weiss, Renaud Morin\thanks{Thanks to XYZ agency for funding.}}
\address{Imactiv-3D, IMT}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
\twoauthors
 {Clément Cazorla, Renaud Morin \sthanks{C. Cazorla is partially funded by ANR CIFRE 2020/0843. He acknowledges the image.sc community for their precious help.}}
	{IMACTIV-3D\\
	1 place Pierre Potier, 31100 - Toulouse}
 {Pierre Weiss \sthanks{P. Weiss acknowledges a support from ANR-3IA Artificial
and Natural Intelligence Toulouse Institute and ANR Micro-Blind}}
	{CNRS \& Université de Toulouse\\
	31400 - Toulouse}

\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
We develop and present a Napari plugin called SVETLANA (SuperVised sEgmenTation cLAssifier for NapAri). It is dedicated to the manual or automatic classification of segmentation results in bio-medical imaging.
While many open-source softwares now make it possible to automatically segment complex 2D and 3D objects such as cells in biology, the subsequent analysis of the results is not yet accessible to non specialists. 
This plugin allows end-users to train and run efficient neural network classifiers such as residual networks. 
The resulting network can be used as a post-processing tool to improve the segmentation, or as a classifier for various tasks (e.g. separating different cell populations).
We showcase its practicality through various real cell biology problems in 2D, 3D and multi-spectral imaging.
\end{abstract}
%
\begin{keywords}
Segmentation, Classification, Convolutional Neural Networks, bio-medical imaging
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

The last decade has made automatic segmentation of bio-medical images much more accessible to users not familiar with signal processing. 
This is the result of progress in machine learning, to the creation of open training databases and to the development of ergonomic open-source softwares. Technologies such as neural networks provide unprecedented segmentation results. They make it possible to avoid setting hyperparameters which are often hard to tune and interpret. 
Examples of powerful and popular tools for segmentation in biology include Ilastik \cite{berg2019ilastik}, CellPose \cite{stringer2021cellpose}, StarDist \cite{fazeli2020automated} or Deep-ImageJ \cite{gomez2021deepimagej}. 

\subsection{Our motivation}

Unfortunately, segmentation masks -- as good as they are -- are rarely directly exploitable to answer biological questions. In particular, it is often necessary to classify the detected objects in order to perform statistical analyses that give a concrete meaning to the results. 
While these excellent segmentation tools have solved an important problem, a difficult part of the analysis remains inaccessible to most users.

\subsection{Our contribution}

The goal of this work is to continue filling the gap between methodological advances and end-users, by providing a convenient software for the classification of segmentation results. 
We resort to the newborn Napari environment \cite{perkel2021python} which allows visualizing and analyzing complex multi-dimensional images (e.g. 2D, 3D, 3D+t, hyperspectral) in Python.
Our software takes the form of a Napari plugin called SVETLANA. 
It is separated in three different modules: 
\begin{description}
  \item[\emph{Annotation}] This module picks connected components of the segmentation masks at random and displays them within their neighborhood, so that the user can label them. In our 2D experiments, it allows labeling about 1000 cells in 15 minutes.
  \item[\emph{Training}] This module allows to pick an arbitrary Pytorch \cite{paszke2019pytorch} neural network architecture (possibly pretrained) and to further train it with the annotations generated by the previous module.
  \item[\emph{Prediction}] This module uses the trained network to classify the connected components of the segmentation mask.
\end{description}
The results are stored in files widely accessible formats for the forth-coming analyzes. This plugin meets a need to further enhance the excellent results obtained with recent wide purpose segmentation tools.

\subsection{Related works}

Different options can be adopted to segment and classify objects in images. 
Before 2010, most of the works relied on the following pipeline (see e.g. \cite{irshad2013methods}): 1) Segment the image 2) Annotate the resulting masks 3) Extract features within the masks (e.g. edges, textures, ...) 4) Design a classifier based on the extracted features. 
Each of the above step was carried out with carefully hand-crafted methods. 
Supervised and unsupervised learning then progressively entered in the game. 
In many cases, they outperformed man-made routines by allowing to explore a wider range of decision routes. 

An effort was then pursued to make these technologies available to the larger number. One remarkable example is Ilastik \cite{berg2019ilastik}. There, a few annotations by the user are usually enough to perform complex classification tasks with an arbitrary number of classes. Its backbone is a random forest classifier with a fixed number of features (convolutions with different filter types). It is widely praised for its ease of use. A few clicks are enough to solve many real-world problems. Unfortunately, this strength is also a limitation in certain cases: the performance of random forests falls short in comparison with the most advanced segmentation and classification routines trained with vast collection of carefully labelled data. 

When precision is critical, the rapidly evolving state-of-the-art is rather based on neural networks and especially convolutional neural networks \cite{dhillon2020convolutional,8237584}. The downside of these technologies is the need to create large data sets, which are usually just not accessible. Each biology laboratory explores a different organism, at a different scale with a different modality and focus. Each collected image can be costly both in terms of money, know-how and time. To address this issue, new initiatives emerge to collect large heterogeneous training databases. For instance the Data Science Bowl \cite{caicedo2019nucleus} allowed to train a single neural network, which is now capable of segmenting cells of nearly any type. This tool, embedded in neat graphical interfaces (e.g. CellPose \cite{stringer2021cellpose} or StarDist \cite{fazeli2020automated}) is a huge asset for biology. Unfortunately, as of now, it does not provide classification tools.


\section{Plugin description}
\label{sec:format}
{}
The objective of SVETLANA as a whole is to provide a tool to "sort" the segmentation results, either manually or automatically. 
It is separated in 3 different stages, similar to what is done in Ilastik \cite{berg2019ilastik}. 

\subsection{The choice of Napari}

Just as Fiji \cite{schindelin2012fiji}, Napari is a completely free and open-source project. The two programs are similar in a number of ways, but we chose Napari for several reasons:
\begin{itemize}
  \item Napari is based on Python and Qt whereas Fiji is based on Java. As we all know, Python is much more efficient than Java for scientific computing. Moreover, it is compatible with all the most recent Deep Learning and optimization libraries which are developed in Python, not in Java. It is also very fast at loading huge images as it supports image pyramids.
  \item The ability to use cookiecutter to generate a plugin template easily makes the integration very pleasant and intuitive. This partly explains the important growth of the number of plugins and the rapid expansion of Napari. Moreover, the developer community is extremely responsive on \url{https://forum.image.sc/} and helped us a lot in the development of Svetlana.
  \item It also already embeds several segmentation reference methods such as Cellpose \cite{stringer2021cellpose} and allows the user to easily load several plugins in series and create his custom processing pipeline, which makes the integration Svetlana natural since it is in line with a segmentation method.
\end{itemize}

\subsection{The annotation mode}

This first sub-plugin takes as input two images: the image itself and its segmentation mask. Each object ot the latter should be given a different label. For instance, a 20-cell image would have a mask with values in a range of 0-20.
\\
There are two philosophies for using this software:
\begin{itemize}
  \item It can be seen as an annotation tool in its own right. For an image with a reasonable amount of segmented objects, it could be used to classify them in a simple and efficient way, in order to perform statistical analyses afterwards.
  \item Nevertheless, if the image contains an astronomical number of detected objects, this software becomes an aid for the creation of a training data set for a classifier. Indeed, the tool extracts a subset of thumbnails of a size chosen by the user and randomly selected in order to maximize the chances of obtaining a minimal and representative data set. 
\end{itemize}

As shown in \ref{labelization}, since it is not easy for the user to determine the optimal patch size, there is a feature to suggest it to the user, as well as the maximum number of thumbnails that can be extracted, i.e. the number of objects of which the segmentation mask is composed. 

It works on 2D and 3D images, whatever the channels number and each patch represents the object in its neighborhood, as shown in \ref{patch}. The user can choose the maximum label number he wants, and each label is attributed clicking on the number we want to give him. As soon as the patch is labeled, the next image is automatically loaded. If an error is committed, a backward step is possible by clicking on the "r" key.

Once the patches have been extracted and the annotation completed, all the information which are necessary for the training are stored in a binary file:
\begin{itemize}
  \item The image path 
  \item The labels image path
  \item The coordinates list for each object
  \item The labels list
  \item The chosen patch size  
\end{itemize}
Additionally, it is possible to save a new mask per existing label, as well as a binary file that can be reloaded using the torch.load() function. It contains morphological features of interest of each of the objects that were extracted from the connected component analysis.


\begin{figure}[htp!]
 \centering
 \includegraphics[scale=0.18]{Figures/annotation.png}
  \caption{Annotation interface}
  \label{labelization}

\end{figure}


\begin{figure}[htp!]
 \centering
 \includegraphics[scale=0.18]{Figures/patch.png}
  \caption{Visualization of a patch to be annotated}
  \label{patch}

\end{figure}



\subsection{The training mode}


Parler de Deep Image Prior (i.e. on peut entraîner avec peu de labels, rasoir d'Ockham, Yann Ollivier).

\subsubsection{The light-weight approach}


For the classification part, we used relatively classical convolutional neural networks such as ResNet18. Nonetheless, the paradigm goes against the classical approach of training neural networks using very large databases.Indeed, we were inspired by the approach put forward in the article Deep Image Prior\cite{lempitsky2018deep} in which they fit a generator network to a single degraded image. They assimilate the reconstruction to a conditional image generation problem and demonstrate that all the information requisite to resolve it is contained in the single degraded input image and in the network chosen for reconstruction.

In section \ref{sec:experiments}, we will show that it is possible to train an efficient classifier model with a few hundred images.

\subsubsection{The features}

In this section, we are going to detail the different features of the training interface shown in figure \ref{training}. The example image shown in the latter is 3D, composed of two channels, and it has been segmented using Cellpose\cite{stringer2021cellpose}.
\begin{figure}[htp!]{}
 \centering
 \includegraphics[scale=0.15]{Figures/training.png}
  \caption{The training sub plugin}
  \label{training}

\end{figure}


First of all, the "load data" button aims at loading the content of the binary file saved at the end of the annotation and whose content we have detailed above.

Then, we propose a wide range of network architectures, but the user can also load the custom neural network of one's choice. We suggest the use of the cross entropy loss, but other classical losses are available. 

The learning rate is chosen at the beginning and there is no scheduler for now, which means it will remain constant the whole training. 

As the possible batch size depends on the performance of the machine the user works on, it can be chosen by the user. If it is selected too large, it will be automatically adjusted to the highest power of 2 tolerated by the GPU ram.

Basic data augmentations such as vertical and horizontal flips and rotations of angle between -90° and 90° are available, and their probability of occurrence can be adjusted. 

Finally, we can choose the name of the ".pth" file containing the training result and all the times we want to save. This gives the possibility to access the network at different stages since we have no preconceived notion of the number of epochs necessary for convergence.

The saved files is also a binary containing several data:
\begin{itemize}
  \item The model
  \item The optimizer (Adam) state
  \item The loss
  \item The epochs number
  \item The loss value list
  \item the image path
  \item The labels image path
  \item The patch size
\end{itemize}



\subsection{The prediction mode}

This plugin is very minimalist, as shown in figure \ref{predict}. Indeed, it allows to load the training result file and to launch the prediction by choosing the batch size. As for the training, the optimal batch size will depend on the capabilities of the GPU.

\begin{figure}[htp!]{}
 \centering
 \includegraphics[scale=0.15]{Figures/resultat.png}
  \caption{Example of a 2D prediction with two labels}
  \label{predict}

\end{figure}


\begin{figure}[h!]{}
 \centering
 \includegraphics[scale=0.15]{Figures/resultat_contour.png}
  \caption{An alternative representation of the labels}
  \label{predict_contours}

\end{figure}

In 2D, there are two representations of the classified mask as shown in figures \ref{predict} and \ref{predict_contours}.

As for the annotation plugin, the user has the opportunity to save a mask per label and a binary file containing relevant morphological information about the classified objects to perform further statistical analysis.


\section{Numerical Experiments}
\label{sec:experiments}


\subsection{First experiment: artificial texture classification}

For this first experiment, we generated an image composed of  345 circular objects differentiable only by their texture that we seek to order into two classes. The distribution of the two textures in the image is about 70/30 and we extract 100 thumbnails to annotate. 
As we have very few data, we decided to demonstrate the efficiency of shallow convolutional networks. As shown in Figure \ref{CNN2D}, we designed a tiny 2-layer CNN of 3000 parameters.

We have a misclassified objects rate of 0.8\%


\begin{figure}[h!]{}
 \centering
 \includegraphics[scale=0.35]{Figures/myfirstcnn.pdf}
  \caption{Our 2-layer custom CNN}
  \label{CNN2D}

\end{figure}


\begin{figure}[h!]
  \begin{center}
    \subfloat[Original texture image]{
      \includegraphics[scale=0.25875]{Figures/textures.png}
      \label{textures}
                         }
    \subfloat[Classified mask]{
      \includegraphics[scale=0.15]{Figures/textures_res.png}
      \label{textures res}
                         }
     \end{center}
\caption{Result of the textures classification}
\end{figure}


\section{Conclusion}
\label{sec:conclusion}




% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
\vfill
\pagebreak

% \section{REFERENCES}
% \label{sec:ref}

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: refs). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
























%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

La dernière décennie a rendu la segmentation automatique d'images bio-médicales bien plus accessible à des utilisateurs non experts en traitement du signal. 
Ceci est largement dû aux progrès en apprentissage automatique et notamment en apprentissage par réseaux de neurones convolutionnels. Ils permettent notamment d'éviter le réglage de nombreux hyperparamètres dont le tuning est difficile et dont la signification est obscure pour les non initiés. 
A titre d'exemple, on peut citer des outils récents, puissants et populaires tels que Ilastik \cite{berg2019ilastik}, Cellpose \cite{stringer2021cellpose}, StarDist \cite{fazeli2020automated} ou plus récemment Deep-ImageJ \cite{gomez2021deepimagej}. 

Malheureusement, les résultats de segmentation -- aussi bons soient-t'ils -- sont rarement exploitables directement pour répondre à des questions biologiques. Il est en effet fréquent qu'il faille  classifier les objets détectés pour effectuer des analyses statistiques apportant un sens concret aux résultats. En un sens, ces outils ont résolu une partie du problème, mais une partie importante et difficile de l'analyse reste inaccessible.

L'objectif de ce travail est de continuer à combler le fossé entre les progrès méthodologiques et les end-users, en fournissant des outils faciles d'utilisation pour la classification des résultats de segmentation.
Ces outils prennent la forme d'un plugin Napari \cite{perkel2021python}, qui est un nouvel outil de visualisation d'images complexes sous Python.

