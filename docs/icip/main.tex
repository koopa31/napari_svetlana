% Template for ICIP-2014 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx,wrapfig}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{SVETLANA: a Supervised Segmentation Classifier for Napari}
%
% - STALIN
% - SALSIFI
% - SALSA
% - SCAN
% 
% ---------------
\name{Clément Cazorla, Pierre Weiss, Renaud Morin\thanks{Thanks to XYZ agency for funding.}}
\address{Imactiv-3D, IMT}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
\twoauthors
 {Clément Cazorla, Renaud Morin \sthanks{C. Cazorla is partially funded by ANR CIFRE 2020/0843. He acknowledges the image.sc community for their precious help.}}
	{IMACTIV-3D\\
	1 place Pierre Potier, 31100 - Toulouse}
 {Pierre Weiss \sthanks{P. Weiss acknowledges a support from ANR-3IA Artificial
and Natural Intelligence Toulouse Institute and ANR Micro-Blind}}
	{CNRS \& Université de Toulouse\\
	31400 - Toulouse}

\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
We develop and present a Napari plugin called SVETLANA (SuperVised sEgmenTation cLAssifier for NapAri). It is dedicated to the manual or automatic classification of segmentation results, with a primary focus on bio-medical imaging.
While many open-source software now make it possible to automatically segment complex 2D and 3D objects such as cells in biology, the subsequent analysis of the results is not yet accessible to non specialists. 
This plugin allows end-users to train and run efficient neural network classifiers such as residual networks. 
The resulting network can be used as a post-processing tool to improve the segmentation, or as a classifier for various tasks (e.g. separating different cells populations).
We showcase its practicality through various real cell biology problems in 2D, 3D and multi-channel imaging.
\end{abstract}
%
\begin{keywords}
Software, Segmentation, Classification, Convolutional Neural Networks, Bio-medical imaging, Image analysis, Microscopy
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

The last decade has made automatic segmentation of bio-medical images much more accessible to users not familiar with signal processing. 
This is the result of progress in machine learning, to the creation of open training databases and to the development of ergonomic open-source software. Technologies such as neural networks provide unprecedented segmentation results. They make it possible to avoid setting hyperparameters which are often hard to tune and interpret. 
Examples of powerful and popular tools for segmentation in biology include Ilastik \cite{berg2019ilastik}, CellPose \cite{stringer2021cellpose}, StarDist \cite{fazeli2020automated} or Deep-ImageJ \cite{gomez2021deepimagej}. 

\subsection{Our motivation}

Unfortunately, segmentation masks -- as good as they are -- are rarely directly exploitable to answer biological questions. In particular, it is often necessary to classify the detected objects in order to perform statistical analyses that give a concrete meaning to the results. 
While these excellent segmentation tools have solved an important problem, a difficult part of the analysis remains inaccessible to most users.

\subsection{Our contribution}

The goal of this work is to continue filling the gap between methodological advances and end-users, by providing a convenient software for the classification of segmentation results. 
We resort to the newborn Napari environment \cite{perkel2021python} which allows visualizing and analyzing complex multi-dimensional images (e.g. 2D, 3D, 3D+t, multi-channel) in Python.
Our software takes the form of a Napari plugin called SVETLANA. A screenshot of the plugin is displayed in Fig. \ref{fig:interface}.

\begin{figure}[htp!]
 \centering
 \includegraphics[width=\linewidth]{Figures/interface.png}
  \caption{The SVETLANA plugin under NAPARI. \label{fig:interface}}
\end{figure}

It takes \emph{an image to label and a segmentation mask} as an input. It is then separated in three different modules depicted in Fig. \ref{fig:pipeline}: 
\begin{description}
  \item[\emph{Annotation}] This module allows to label connected components of the segmentation masks. In our 2D experiments, it allows labeling about 1000 connected component in less than 30 minutes.
  \item[\emph{Training}] This module allows to pick an arbitrary Pytorch \cite{paszke2019pytorch} neural network architecture (possibly pretrained) and to further train it with the annotations generated by the previous module.
  \item[\emph{Prediction}] This module uses the trained network to classify the connected components of the segmentation mask.
\end{description}
The outputs of the plugin are: \emph{a set of manually annotated patches, a trained neural network and a prediction mask}. 
The results are stored in files widely accessible formats for the forth-coming analyzes. This plugin meets a need to further enhance the excellent results obtained with recent, wide purpose segmentation tools. 

\begin{figure*}[htp!]
 \centering
 \includegraphics[width=\linewidth]{Figures/pipeline_svetlana.pdf}
  \caption{The SVETLANA plugin pipeline. \label{fig:pipeline}}
\end{figure*}


\subsection{Related works}

Different options can be adopted to segment and classify objects in images. 
Before 2010, most of the works relied on the following pipeline (see e.g. \cite{irshad2013methods}): 1) Segment the image 2) Annotate the resulting masks 3) Extract features within the masks (e.g. edges, textures, ...) 4) Design a classifier based on the extracted features. 
Each of the above step was carried out with carefully hand-crafted methods. 
Supervised and unsupervised learning then progressively entered in the game. 
In many cases, they outperformed man-made routines by allowing to explore a wider range of decision routes. 

An effort was then pursued to make these technologies available to the larger number. One remarkable example is Ilastik \cite{berg2019ilastik}. There, a few annotations by the user are usually enough to perform complex classification tasks with an arbitrary number of classes. Its backbone is a random forest classifier with a fixed number of features (convolutions with different filter types). It is widely praised for its ease of use. A few clicks are enough to solve many real-world problems. Unfortunately, this strength is also a limitation in certain cases: the performance of random forests falls short in comparison with the most advanced segmentation and classification routines trained with vast collection of carefully labeled data. 

When precision is critical, the rapidly evolving state-of-the-art is rather based on neural networks and especially convolutional neural networks \cite{dhillon2020convolutional,8237584}. The downside of these technologies is the need to create large data sets, which are usually just not accessible. Each biology laboratory explores a different organism, at a different scale with a different modality and focus. Each collected image can be costly both in terms of money, know-how and time. To address this issue, new initiatives emerge to collect large heterogeneous training databases. For instance the Data Science Bowl \cite{caicedo2019nucleus} allowed to train a single neural network, which is now capable of segmenting cells of nearly any type. This tool, embedded in neat graphical interfaces (e.g. CellPose \cite{stringer2021cellpose} or StarDist \cite{fazeli2020automated}) is a huge asset for biology. Unfortunately, as of now, it does not provide classification tools.

The proposed ideas can be seen as a two-step procedure to classify complex objects. First, we use a general purpose segmentation routine trained with a huge dataset. This highly simplifies the classification task by having to focus only on the right objects. Then train a classifier with just a few annotations (e.g. 100-1000) to sort the resulting objects. As far as we are aware of, this principle is not yet made available as an open-source software.

\section{Plugin description}
\label{sec:format}

The objective of SVETLANA as a whole is to provide a user-friendly tool targeted to people not familiar with programming to label segmentation results, either manually or automatically. 

\subsection{The choice of Napari}
The specifications were:
\begin{itemize}
  \setlength{\itemsep}{3pt}%
  \setlength{\parskip}{0pt}%
  \item Use highly parallel architectures for run-time efficiency.
  \item Easyness to integrate new classifiers.
  \item Embbed in an environment providing efficient segmentation tools.
  \item Embbed in an environment allowing to visualize the results efficiently.
  \item Easyness to use other analysis tools.
\end{itemize}
Those considerations led us to choose the Napari environment. 
It is developed in Python and already includes state-of-the-art segmentation tools such as Cellpose \cite{stringer2021cellpose}. 
In addition, the development is currently very fast with new plugins being issued on a weekly basis.
It allows to interact with Pytorch or TensorFlow in a natural way and therefore use the latest developments in machine learning.

\subsection{The plugin}

A general overview of the plugin is provided in Fig. \ref{fig:pipeline}. 

\subsubsection{The annotation mode}

% \begin{figure}
%   \centering
%   \includegraphics[width=0.5\linewidth]{Figures/annotation.png}
%   \caption{Annotation module. \label{fig:patch}}
% \end{figure}

This first sub-plugin takes two images as an input: the image itself and its segmentation mask. The user can also choose the number of different labels he wants.

The connected components of the segmentation mask are then extracted using scikit-image \cite{van2014scikit}. 
Then, there are two possible ways to annotate: 
\begin{itemize}
  \setlength{\itemsep}{3pt}%
  \setlength{\parskip}{0pt}%
  \item The user clicks on the desired connected component and labels it. 
  \item SVETLANA randomly picks a connected component, displays it with its neighborhood and the user labels it. This mode can help avoiding a user bias in the choice of the connected components.
\end{itemize}

Once the user feels enough annotations have been proposed, he can save the results in a binary file.
%  including the image path, the labels image path, the list of coordinates for each connected component and the associated labels.
% Additionally, it is possible to save a new mask per existing label, as well as a binary file that can be reloaded using the torch.load() function. It contains morphological features of interest of each of the objects that were extracted from the connected component analysis.

\subsubsection{The training mode}

\paragraph*{Interface}

In this mode, the user can:
\begin{itemize}
  \setlength{\itemsep}{3pt}%
  \setlength{\parskip}{0pt}%
 \item choose a loss function (cross-entropy, binary cross-entropy, MSE,...)
 \item pick a neural network from a list of pre-defined architectures (e.g. ResNet with various depths \cite{7780459}) or his own pre-trained network in a .pth file.
 \item adapt the parameters of the Adam optimization routine.
 \item choose the batch-size for training, depending on the memory resource available.
 \item choose the type of data augmentation he wants (none, flip, rotation, \ldots)
 \item train the network with the previous parameters.
\end{itemize}

\paragraph*{Why could it work?}

Training a classifier with just a few annotations (10-1000) goes against conventional wisdom. Indeed, complex neural network architectures as ResNet are usually trained with huge dataset. It is therefore legitimate to question why the training could yield a good classifier. 
We do not have a theoretical answer for this beyond numerical experiments. 
It turns out that this approach provides fairly good classifiers (see section \ref{sec:experiments}).

Let us mention that a few recent works point out that this is a rich research avenue. 
An example is Deep Image Prior \cite{lempitsky2018deep}. 
There, it was shown that a large collection of linear inverse problems could be solved by using a single image as an input without any training. 
One way to interpret this is that the training phase could act as Occam razor's principle and favor the 'simplest' answer given the observations.
The network architecture plays a critical there and we observed experimentally that simple networks with few weights were often preferable to more complicated ones.
In all forthcoming experiments, we used the simple 2-layer neural net depicted in Fig. \ref{fig:CNN2D}. It contains 3017 parameters.
\begin{figure}[h!]{}
 \centering
 \includegraphics[width=0.7\linewidth]{Figures/myfirstcnn.pdf}
  \caption{A minimalist 2-layer convolutional neural network. \label{fig:CNN2D}}
\end{figure}


% \paragraph*{The features}

% In this section, we are going to detail the different features of the training interface shown in figure \ref{training}. The example image shown in the latter is 3D, composed of two channels, and it has been segmented using Cellpose\cite{stringer2021cellpose}.
% % \begin{figure}[htp!]{}
% %  \centering
% %  \includegraphics[scale=0.15]{Figures/training.png}
% %   \caption{The training sub plugin}
% %   \label{training}
% % \end{figure}


% First of all, the "load data" button aims at loading the content of the binary file saved at the end of the annotation and whose content we have detailed above.

% Then, we propose a wide range of network architectures, but the user can also load the custom neural network of one's choice. We suggest the use of the cross entropy loss, but other classical losses are available. 

% The learning rate is chosen at the beginning and there is no scheduler for now, which means it will remain constant the whole training. 

% As the possible batch size depends on the performance of the machine the user works on, it can be chosen by the user. If it is selected too large, it will be automatically adjusted to the highest power of 2 tolerated by the GPU ram.

% Basic data augmentations such as vertical and horizontal flips and rotations of angle between -90° and 90° are available, and their probability of occurrence can be adjusted. 

% Finally, we can choose the name of the ".pth" file containing the training result and all the times we want to save. This gives the possibility to access the network at different stages since we have no preconceived notion of the number of epochs necessary for convergence.

% The saved files is also a binary containing several data including: the model, the optimizer state, the loss, the epochs number, the loss value, the image path, the labels image path, the patch size.

\subsubsection{The prediction mode}

This plugin allows to load the trained network and to launch the prediction by choosing the batch size. It then saves the results in a binary file.

% \begin{figure}[htp!]{}
%  \centering
%  \includegraphics[scale=0.15]{Figures/resultat.png}
%   \caption{Example of a 2D prediction with two labels}
%   \label{predict}

% \end{figure}


% \begin{figure}[h!]{}
%  \centering
%  \includegraphics[scale=0.15]{Figures/resultat_contour.png}
%   \caption{An alternative representation of the labels}
%   \label{predict_contours}
% \end{figure}


\section{Numerical Experiments}
\label{sec:experiments}

In all experiments, we use the minimalist CNN from Fig. \ref{fig:CNN2D}. 
For the data augmentation, we simply flipped the patches in the 4 arbitrary positions.
All calculations were performed on an Nvidia RTX5000 with 16Go memory.

\subsection{Experiment 1: artificial texture classification}

For this first experiment, we generated an image composed of  345 circular objects with two different textures (see Fig. \ref{fig:textures}). 
For the training, we label 100 objects in about 3 minutes. 
After a training 500 epochs (about 20 seconds), we get a misclassification rate of 0.8\%.

\begin{figure}[h!]
  \begin{center}
    \subfloat[Original image]{\includegraphics[width=0.45\linewidth]{Figures/textures.png}} \quad 
    \subfloat[Classification]{\includegraphics[width=0.45\linewidth]{Figures/textures_res.png}}
     \end{center}
\caption{A first result on texture classification. \label{fig:textures}}
\end{figure}


\subsection{Experiment 2: Infected cells detection in virology}

The two images on the left of Fig. \ref{fig:infected} depict two different channels of human embryonic kidney (HEK) cells. 
Some of them are infected with an adenovirus. 
The first channel (left) displays a Hoechst staining of the nuclei.
In the second channel, the sites of viral proliferation have been marked using the ANCHOR technology \cite{mariame2018real}.
The cells containing bright nuclear spots in the second channel are more likely to be infected.
We annotated 100 cells over 245 (2 minutes) to train the 2-layer model and obtained 95.5\% of well-classified cells.

\begin{figure}[h!]
  \begin{center}
    \subfloat[Hoechst staining]{
      \includegraphics[width=0.3\linewidth]{Figures/viro_canal0.png}
      \label{textures}
    }
    \subfloat[ANCHOR channel]{
      \includegraphics[width=0.3\linewidth]{Figures/viro_canal1.png}
      \label{textures res}
    }
   \subfloat[classification result]{
      \includegraphics[width=0.3\linewidth]{Figures/resultat_viro.png}
      \label{textures res}
      }
     \end{center}
\caption{Result of the infected cells classification. The image was acquired and provided to us by \href{https://neovirtech.com/}{Neovirtech}. \label{fig:infected}}
\end{figure}


\subsection{Experiment 3: Osteoclast classification}

In this experiment, we wish to better understand and quantify osteoclastogenesis in cell culture.
To this end, we quantify the density of activated osteoclasts, which allows detecting skeletal abnormalities such as osteoporosis.

\begin{figure}[h!]
  \begin{center}
    \subfloat[Bright-field image]{\includegraphics[width=0.45\linewidth]{Figures/crop_ABS.png}}
    \subfloat[Classification result]{\includegraphics[width=0.45\linewidth]{Figures/crop_mask.png}}\\
    \subfloat[Label 1]{\includegraphics[width=0.22\linewidth]{Figures/good1.png}} \ 
    \subfloat[Label 1]{\includegraphics[width=0.22\linewidth]{Figures/good2.png}} \ 
    \subfloat[Label 2]{\includegraphics[width=0.22\linewidth]{Figures/bad1.png}} \ 
    \subfloat[Label 2]{\includegraphics[width=0.22\linewidth]{Figures/bad2.png}} \ 
  \end{center}
  \caption{Left: $750\times 750$ crop of a $8000\times 8000$ image. Right: classification result with a training on 600 cells (out of 16671).
The images and the problem was brought to us by \href{https://www.atlantic-bone-screen.com/language/en/osteoclasts-2/}{Atlantic Bone Screen (ABS).}}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}

We presented a new software called SVETLANA for the classification of segmentation results within the Napari environment. 
We showed through various applications that SVETLANA is a handful tool for the analysis and classification of cell populations. 
This software is yet at an early stage of development and additional features will be added with user feedback.
It is accessible on \href{https://bitbucket.org/koopa31/napari_svetlana/src/main/}{BitBucket} installed with \begin{verbatim}pip install napari_svetlana\end{verbatim}. 
This paper is a way to advertise this work, which we hope will prove valuable to the bio-medical (and beyond) community. 

% \begin{itemize}
%   \item Add the ability to label multiple images. 
% %  \item Add the ability to directly label or correcting the labels by clicking on the connected components. 
%   \item Add the ability to predict on multiple images. 
% %  \item Add new neural network architectures. 
% %  \item Add a layer with already annotated features.
% \end{itemize}

% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
\vfill
\pagebreak

% \section{REFERENCES}
% \label{sec:ref}

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: refs). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
























%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

La dernière décennie a rendu la segmentation automatique d'images bio-médicales bien plus accessible à des utilisateurs non experts en traitement du signal. 
Ceci est largement dû aux progrès en apprentissage automatique et notamment en apprentissage par réseaux de neurones convolutionnels. Ils permettent notamment d'éviter le réglage de nombreux hyperparamètres dont le tuning est difficile et dont la signification est obscure pour les non initiés. 
A titre d'exemple, on peut citer des outils récents, puissants et populaires tels que Ilastik \cite{berg2019ilastik}, Cellpose \cite{stringer2021cellpose}, StarDist \cite{fazeli2020automated} ou plus récemment Deep-ImageJ \cite{gomez2021deepimagej}. 

Malheureusement, les résultats de segmentation -- aussi bons soient-t'ils -- sont rarement exploitables directement pour répondre à des questions biologiques. Il est en effet fréquent qu'il faille  classifier les objets détectés pour effectuer des analyses statistiques apportant un sens concret aux résultats. En un sens, ces outils ont résolu une partie du problème, mais une partie importante et difficile de l'analyse reste inaccessible.

L'objectif de ce travail est de continuer à combler le fossé entre les progrès méthodologiques et les end-users, en fournissant des outils faciles d'utilisation pour la classification des résultats de segmentation.
Ces outils prennent la forme d'un plugin Napari \cite{perkel2021python}, qui est un nouvel outil de visualisation d'images complexes sous Python.

